
14/3/2022:
input layer-> first activation layer
hidden layer-> next activation layer
output layer-> last activation layer
a^l_i:= i is node order, l is layer
*vectorize vector:
-> allignment x as column vt and w as row vt
activation function:
a[i]=g(z[i])
g can be many fuction
g= tanh (-1<y<1)
g= sigmoid (0<y<1)
rcm output only =sigmoid
g= ReLU =max(0,z) (most common)
g= leaky ReLU=max(0.01z,z)
NOT to put many linear at once -> similar to 1 linear regression
*derivatives of activation
sigmoid'=sigmoid(1-sigmoid)
tanh'=1-tanh^2
ReLU=0 or 1
LeakyReLU=0.01 or 1
* gradient descent
EG: single hidden layer
forward  popagation
z2=w2 a1+b2
a2=g(z2)=sigmoid(z2)
z1=w1 x+b1
a1=g(z1)
back popagation
dz2=dj/dz=a2-y
dw2=1/m np.dot(dz2,a1.T)
db = 1/m np.sum(dz,axis=1,keepdims=true)
dz= np.dot(w2,dz2)* g' (element-wide product)
dw1=1/m np.dot(dz1,x.T)
db1= 1/m np.sum(dz1, axis=1,keepdims=true)
!keepdims keep 2 dimension array
*random init 
np.random.rand((m,n))* small_rate
*deep NN
L=4 = number of layers
n[l] units of layer l
wl=
* foward backward
a0->[w1,b1]-a1-->[w2,b2]-...->[wi,bi]->a[i](y^)
	|z1	   |z2		|z3	|zi
    [w1,b1]<-da1-[w2,b2]<-...-[wi,bi]<-dai
	|	    |		|		
	dw1	   dw2		dwi	
	db1	   db2		dbi
*bias/variance
high bias: poor performance on train set
high variance: poor performance on test set/ overfitting
* road for ML
high bias-y- change network(nodes, architecture)- repeat
|
n
|
high variance-y- change data, regularization, architecture- repeat
*regulization:
L1 regularization: L+lambda*||w||1/m
L2 regularization: L+lambda*||w||2/m
frobenius norm (nn): sum of square of elements in matrix
update dw after add regularization
dw[l]=lambda*w[l]/m+back_prop
update new w[l]:w[l]:=(1-lr*lambda/m)*w[l]-lr*(back_prop) (weight decay)
lambda big->  w[l] ~0
reduce nodes
*drop out regularization
0.5 chance of remove nodes
implement:
+example: layer=3, keep_pron=0.8
+d3=np.random,rand(a3.shape[0], a3.shape[1]) < keep_prob --> d3= matrix contain 0 or 1
+a3=a3*d3
+a3/= keep_prob (z[4]=w[4].a[3]+b[4], a[3] reduced *=(1-keep_prob)-> /=keep_prob)
*explaination
* early stopping:
dev set error & training set error good enough for the iteration (dev set error getting worse)
*normalize set:
subtract means
devide norm
--> symetric
*vanishing/exploding gradients
multiple multi weight in layers create function of power
--> init weights: reduce in vanish/explode
ReLU: var=(1/n)  --> w[l]= rand(shape)*sqrt(1/n[l])
tanh: var=(2/n) 
*grad check
wi,bi -> theta_i
*mini-batch
size ~ >1 and <all
ideal : 2^k (k= 6;10)
*exponentially weight average
v_t=beta* v_(t-1)+(1-beta)*theta_t
meaning: avg value over 1 period
v_t=sigma(beta*(1-beta)^k*v_(t-k))
--> graph will be smoothier
-problem: t low (t<1/(1-beta))--> v_t~0
- solve: v_t=vt/(1-beta^t)
*GD with momentum
-do ewa to dw, db
-implement: vdw := beta*(vdw)+(1-beta)*dw; w:=w-lr*vdw
same with b
*RMSprop
- Sdw= beta*Sdw+(1-beta)* dw^2
- w= w-lr*dw/sqrt(Sdw +e )   #e is preventing sdw close to 0
same with b
*adam opt algo
Sdw= (beta_2*Sdw+(1-beta_2)*dw^2)/(1-beta_2^t)
Vdw= (beta_1*Vdw+(1-beta_1)*dw)/(1-beta_1^t)
w= w-lr*Vdw/sqrt(Sdw+e)
beta_1 ~0.9
beta_2~0.999
e~ 10^-8
* lr decay
- lr_i=lr_0/(1+decay_rate*i)
- lr_i=0.95^i

* batch norm
z=gamma*(z-mu)/sqrt(epsi+sigma)+beta
-note that gamma and beta are learnable

*softmax regression
- output layer has >1 output (size =(n,1))
- activation function for softmax layer: 
+ z=w*a+b (4,1)
+ t= e^z
+ a=t/np.sum(t)
- lost func: J=1/m*sum (-ylog(a_l))
*orthogonalization (90 degree each dimension)
- seperately change/tunning during stages of training/dev/test sets
*error ananlysts
- mislabeled  (but correct with purpose like cat but labeled wrong) high-> potential high
- DL accept the random error (typo,etc) if considerable ratio in total error -> fix, else, can ignore
- test/dev set different distribution:
+ opt1: merge both -> proportion from test set / dev set will not work as what we want (target)
+opt2: dev/test will be the target purpose 
-bias and variance:
+ train-dev: same distributtion to the train but not learn, if high level of error -> high varience problem, if low level error -> mismatch, if train set have high error from human leve-> bias
+
