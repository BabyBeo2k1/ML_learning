
14/3/2022:
input layer-> first activation layer
hidden layer-> next activation layer
output layer-> last activation layer
a^l_i:= i is node order, l is layer
*vectorize vector:
-> allignment x as column vt and w as row vt
activation function:
a[i]=g(z[i])
g can be many fuction
g= tanh (-1<y<1)
g= sigmoid (0<y<1)
rcm output only =sigmoid
g= ReLU =max(0,z) (most common)
g= leaky ReLU=max(0.01z,z)
NOT to put many linear at once -> similar to 1 linear regression
*derivatives of activation
sigmoid'=sigmoid(1-sigmoid)
tanh'=1-tanh^2
ReLU=0 or 1
LeakyReLU=0.01 or 1
* gradient descent
EG: single hidden layer
forward  popagation
z2=w2 a1+b2
a2=g(z2)=sigmoid(z2)
z1=w1 x+b1
a1=g(z1)
back popagation
dz2=dj/dz=a2-y
dw2=1/m np.dot(dz2,a1.T)
db = 1/m np.sum(dz,axis=1,keepdims=true)
dz= np.dot(w2,dz2)* g' (element-wide product)
dw1=1/m np.dot(dz1,x.T)
db1= 1/m np.sum(dz1, axis=1,keepdims=true)
!keepdims keep 2 dimension array
*random init 
np.random.rand((m,n))* small_rate
*deep NN
L=4 = number of layers
n[l] units of layer l
wl=
* foward backward
a0->[w1,b1]-a1-->[w2,b2]-...->[wi,bi]->a[i](y^)
	|z1	   |z2		|z3	|zi
    [w1,b1]<-da1-[w2,b2]<-...-[wi,bi]<-dai
	|	    |		|		
	dw1	   dw2		dwi	
	db1	   db2		dbi
*bias/variance
high bias: poor performance on train set
high variance: poor performance on test set/ overfitting
* road for ML
high bias-y- change network(nodes, architecture)- repeat
|
n
|
high variance-y- change data, regularization, architecture- repeat
*regulization:
L1 regularization: L+lambda*||w||1/m
L2 regularization: L+lambda*||w||2/m
frobenius norm (nn): sum of square of elements in matrix
update dw after add regularization
dw[l]=lambda*w[l]/m+back_prop
update new w[l]:w[l]:=(1-lr*lambda/m)*w[l]-lr*(back_prop) (weight decay)
lambda big->  w[l] ~0
reduce nodes
*drop out regularization
0.5 chance of remove nodes
implement:
+example: layer=3, keep_pron=0.8
+d3=np.random,rand(a3.shape[0], a3.shape[1]) < keep_prob --> d3= matrix contain 0 or 1
+a3=a3*d3
+a3/= keep_prob (z[4]=w[4].a[3]+b[4], a[3] reduced *=(1-keep_prob)-> /=keep_prob)
*explaination
* early stopping:
dev set error & training set error good enough for the iteration (dev set error getting worse)
*normalize set:
subtract means
devide norm
--> symetric
*vanishing/exploding gradients
multiple multi weight in layers create function of power
--> init weights: reduce in vanish/explode
ReLU: var=(1/n)  --> w[l]= rand(shape)*sqrt(1/n[l])
tanh: var=(2/n) 
*grad check
wi,bi -> theta_i
*mini-batch
size ~ >1 and <all
ideal : 2^k (k= 6;10)
*exponentially weight average
v_t=beta* v_(t-1)+(1-beta)*theta_t
meaning: avg value over 1 period
v_t=sigma(beta*(1-beta)^k*v_(t-k))
--> graph will be smoothier
-problem: t low (t<1/(1-beta))--> v_t~0
- solve: v_t=vt/(1-beta^t)
*GD with momentum
-do ewa to dw, db
-implement: vdw := beta*(vdw)+(1-beta)*dw; w:=w-lr*vdw
same with b
*RMSprop
- Sdw= beta*Sdw+(1-beta)* dw^2
- w= w-lr*dw/sqrt(Sdw +e )   #e is preventing sdw close to 0
same with b
*adam opt algo
Sdw= (beta_2*Sdw+(1-beta_2)*dw^2)/(1-beta_2^t)
Vdw= (beta_1*Vdw+(1-beta_1)*dw)/(1-beta_1^t)
w= w-lr*Vdw/sqrt(Sdw+e)
beta_1 ~0.9
beta_2~0.999
e~ 10^-8
* lr decay
- lr_i=lr_0/(1+decay_rate*i)
- lr_i=0.95^i

* batch norm
z=gamma*(z-mu)/sqrt(epsi+sigma)+beta
-note that gamma and beta are learnable

*softmax regression
- output layer has >1 output (size =(n,1))
- activation function for softmax layer: 
+ z=w*a+b (4,1)
+ t= e^z
+ a=t/np.sum(t)
- lost func: J=1/m*sum (-ylog(a_l))
*orthogonalization (90 degree each dimension)
- seperately change/tunning during stages of training/dev/test sets
*error ananlysts
- mislabeled  (but correct with purpose like cat but labeled wrong) high-> potential high
- DL accept the random error (typo,etc) if considerable ratio in total error -> fix, else, can ignore
- test/dev set different distribution:
+ opt1: merge both -> proportion from test set / dev set will not work as what we want (target)
+opt2: dev/test will be the target purpose 
-bias and variance:
+ train-dev: same distributtion to the train but not learn, if high level of error -> high varience problem, if low level error -> mismatch, if train set have high error from human leve-> bias
-Transfer learning
+ some layer transfer for do other things. eg: a cat detection use for the radiation detect
+ transfer only from a nn that has >> target nn
+  transfer through same type input (img)
-multi-task
+ last layer has n nodes for number of target task
+ share same input (4 objects in 1 img)
+similar input
*end-to-end DL
- suit with big data 
- skip processing data (eg: voice-> wwords-> phonemes-> text)
-pros&cons
+ purely reflect the data without preconception
+ less hand design
+ need large quantity of data
+ miss useful hand-designed components
- have sufficient dât to learn function of the complexity needed 
(cần đủ dữ liệu cần thiết để có thể học được độ phức tạp của hàm maping x-y )
*CNN
-egde detection:
convolution: eg: nxn(a) * mxm(b or called filter) at (i,j), mxm matrix with (1,1) is (i,j) of a, multiple elementwise with b sum up all, then put at (i,j), nxn * mxm should be (n-m+1)x(n-m+1)
tf.nn.conv2d
+ vertical edge detection: 
1 0 -1
1 0 -1
1 0 -1
+horizontal edge detection:
1  1  1
0  0  0
-1 -1 -1 
- padding: adding border around image that can help prevent loss information and shrink output (p= thickness of border)
- valid convol: p=0
-same convol: p=(f-1)/2
--> f mostly odd
-stride convol: distance between next convol multiple maxtrix,
eg: 1,1 at res= 1,1 3x3 matrix * filter; 1,2 at res = 1,3 matrix * filter (stride=2)
--> res is ((n+2p-f)/s+1)x((n+2p-f)/s+1) matrix
-cross-corelation - flip filter
-convol on rgb: filter mxmx3 
-multiple filter: convol with different filter-> stack on 3d matrix
* 1 layer of convol nn:
input*filter_i-> res_i
g(res+b_i) ->stack all -> a1 (g is non-linear)
* convNet example:
39x39x3-p=0,s=1,f=3,filter=10-> 37x37x10-p=0,s=2,f=5,filter=20-> 17x17x20...-> 7x7x40-> array 1960 mem-> softmax
* pooling layer
-max pooling: divide matrix to smaller same size matrix, choose max. eg: 4x4  max pooling to 2x2 by use max member in sub-matrix 2x2 of the first matrix
-avg pooling: avg of sub-matrix
common hyperparameters: f=2, s=2
*fully connected layer:
after pooling-> spread data in a array-> next layer act like normal layer= FC layer
*cnn 
conv-pool -> conv-pool-> 
