
14/3/2022:
input layer-> first activation layer
hidden layer-> next activation layer
output layer-> last activation layer
a^l_i:= i is node order, l is layer
*vectorize vector:
-> allignment x as column vt and w as row vt
activation function:
a[i]=g(z[i])
g can be many fuction
g= tanh (-1<y<1)
g= sigmoid (0<y<1)
rcm output only =sigmoid
g= ReLU =max(0,z) (most common)
g= leaky ReLU=max(0.01z,z)
NOT to put many linear at once -> similar to 1 linear regression
*derivatives of activation
sigmoid'=sigmoid(1-sigmoid)
tanh'=1-tanh^2
ReLU=0 or 1
LeakyReLU=0.01 or 1
* gradient descent
EG: single hidden layer
forward  popagation
z2=w2 a1+b2
a2=g(z2)=sigmoid(z2)
z1=w1 x+b1
a1=g(z1)
back popagation
dz2=dj/dz=a2-y
dw2=1/m np.dot(dz2,a1.T)
db = 1/m np.sum(dz,axis=1,keepdims=true)
dz= np.dot(w2,dz2)* g' (element-wide product)
dw1=1/m np.dot(dz1,x.T)
db1= 1/m np.sum(dz1, axis=1,keepdims=true)
!keepdims keep 2 dimension array
*random init 
np.random.rand((m,n))* small_rate
*deep NN
L=4 = number of layers
n[l] units of layer l
wl=
* foward backward
a0->[w1,b1]-a1-->[w2,b2]-...->[wi,bi]->a[i](y^)
	|z1	   |z2		|z3	|zi
    [w1,b1]<-da1-[w2,b2]<-...-[wi,bi]<-dai
	|	    |		|		
	dw1	   dw2		dwi	
	db1	   db2		dbi
*bias/variance
high bias: poor performance on train set
high variance: poor performance on test set/ overfitting
* road for ML
high bias-y- change network(nodes, architecture)- repeat
|
n
|
high variance-y- change data, regularization, architecture- repeat
*regulization:
L1 regularization: L+lambda*||w||1/m
L2 regularization: L+lambda*||w||2/m
frobenius norm (nn): sum of square of elements in matrix
update dw after add regularization
dw[l]=lambda*w[l]/m+back_prop
update new w[l]:w[l]:=(1-lr*lambda/m)*w[l]-lr*(back_prop) (weight decay)
lambda big->  w[l] ~0
reduce nodes
*drop out regularization
0.5 chance of remove nodes
implement:
+example: layer=3, keep_pron=0.8
+d3=np.random,rand(a3.shape[0], a3.shape[1]) < keep_prob --> d3= matrix contain 0 or 1
+a3=a3*d3
+a3/= keep_prob (z[4]=w[4].a[3]+b[4], a[3] reduced *=(1-keep_prob)-> /=keep_prob)
*explaination
* early stopping:
dev set error & training set error good enough for the iteration (dev set error getting worse)
*normalize set:
subtract means
devide norm
--> symetric
*vanishing/exploding gradients
multiple multi weight in layers create function of power
--> init weights: reduce in vanish/explode
ReLU: var=(1/n)  --> w[l]= rand(shape)*sqrt(1/n[l])
tanh: var=(2/n) 
*grad check
wi,bi -> theta_i
dtheta_i=