14/3/2022:
input layer-> first activation layer
hidden layer-> next activation layer
output layer-> last activation layer
a^l_i:= i is node order, l is layer
*vectorize vector:
-> allignment x as column vt and w as row vt
activation function:
a[i]=g(z[i])
g can be many fuction
g= tanh (-1<y<1)
g= sigmoid (0<y<1)
rcm output only =sigmoid
g= ReLU =max(0,z) (most common)
g= leaky ReLU=max(0.01z,z)
NOT to put many linear at once -> similar to 1 linear regression
*derivatives of activation
sigmoid'=sigmoid(1-sigmoid)
tanh'=1-tanh^2
ReLU=0 or 1
LeakyReLU=0.01 or 1
* gradient descent
EG: single hidden layer
forward  popagation
z2=w2 a1+b2
a2=g(z2)=sigmoid(z2)
z1=w1 x+b1
a1=g(z1)
back popagation
dz2=dj/dz=a2-y
dw2=1/m np.dot(dz2,a1.T)
db = 1/m np.sum(dz,axis=1,keepdims=true)
dz= np.dot(w2,dz2)* g' (element-wide product)
dw1=1/m np.dot(dz1,x.T)
db1= 1/m np.sum(dz1, axis=1,keepdims=true)
!keepdims keep 2 dimension array
*random init 
np.random.rand((m,n))* small_rate
*deep NN
L=4 = number of layers
n[l] units of layer l
wl=
* foward backward
a0->[w1,b1]-a1-->[w2,b2]-...->[wi,bi]->a[i](y^)
	|z1	   |z2		|z3	|zi
    [w1,b1]<-da1-[w2,b2]<-...-[wi,bi]<-dai
	|	    |		|		
	dw1	   dw2		dwi	
	db1	   db2		dbi
